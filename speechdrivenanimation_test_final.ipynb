{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import houndify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../speech-driven-animation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import time\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: what time\n",
      "Partial transcript: what time is\n",
      "Partial transcript: what time is\n",
      "Partial transcript: what time is it\n",
      "Partial transcript: what time is it in\n",
      "Partial transcript: what time is it in\n",
      "Partial transcript: what time is it in de\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Partial transcript: what time is it in denver\n",
      "Final response: {'BuildInfo': {'Variant': 'release', 'User': 'knightly', 'BuildNumber': '6107', 'Kind': 'Low Fat', 'SVNRevision': '54997', 'SVNBranch': 'dev_multi_machine', 'Machine': '7fhpjh2.pnp.melodis.com', 'Date': 'Sun Dec  8 20:17:31 PST 2019'}, 'FormatVersion': '1.0', 'AllResults': [{'WrittenResponseLong': \"Didn't get that!\", 'AutoListen': False, 'SpokenResponseLong': \"Didn't get that!\", 'ViewType': ['Native', 'None'], 'CommandKind': 'NoResultCommand', 'TranscriptionSearchURL': 'http://www.google.com/#q=what%20time%20is%20it%20in%20denver', 'WrittenResponse': \"Didn't get that!\", 'SpokenResponse': ''}], 'Status': 'OK', 'DomainUsage': [], 'AudioLength': 2.230000019073486, 'ServerGeneratedId': '28d33063-b3cc-2a3f-b54d-0b9adefb9fbc', 'Format': 'SoundHoundVoiceSearchResult', 'NumToReturn': 1, 'ResultsAreFinal': [True], 'Disambiguation': {'NumToShow': 1, 'ChoiceData': [{'Transcription': 'what time is it in denver', 'ConfidenceScore': 0.6859999999999999, 'FixedTranscription': '', 'FormattedTranscription': 'what time is it in denver'}]}, 'QueryID': '28d33063-b3cc-2a3f-b54d-0b9adefb9fbc', 'RealTime': 1, 'RealSpeechTime': 0.997}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/home/friend/notebooks/houndify.py\", line 362, in _callback\n",
      "    listener.onFinalResponse(parsedMsg)\n",
      "  File \"<ipython-input-13-f468c31ff162>\", line 34, in onFinalResponse\n",
      "    response_bytes = response[\"AllResults\"][0][\"ResponseAudioBytes\"]\n",
      "KeyError: 'ResponseAudioBytes'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLIENT_ID = \"Rco-d2ARwyAUSvklEH_ePg==\"\n",
    "CLIENT_KEY = \"V6TzSgIOkx0CW4AkBSxgLNrcHMEZ26PSB4uLaG-n8t6PsjZSEzBF-aGfXQv-mWXha0_grVB-TO5MGdKhx9XNFA==\"\n",
    "AUDIO_FILE = \"whattimeisitindenver.wav\"\n",
    "\n",
    "BUFFER_SIZE = 256\n",
    "\n",
    "audio = wave.open(AUDIO_FILE)\n",
    "if audio.getsampwidth() != 2:\n",
    "  print(\"%s: wrong sample width (must be 16-bit)\" % fname)\n",
    "  sys.exit()\n",
    "if audio.getframerate() != 8000 and audio.getframerate() != 16000:\n",
    "  print(\"%s: unsupported sampling frequency (must be either 8 or 16 khz)\" % fname)\n",
    "  sys.exit()\n",
    "if audio.getnchannels() != 1:\n",
    "  print(\"%s: must be single channel (mono)\" % fname)\n",
    "  sys.exit()\n",
    "\n",
    "\n",
    "audio_size = audio.getnframes() * audio.getsampwidth()\n",
    "audio_duration = audio.getnframes() / audio.getframerate()\n",
    "chunk_duration = BUFFER_SIZE * audio_duration / audio_size\n",
    "\n",
    "\n",
    "#\n",
    "# Simplest HoundListener; just print out what we receive.\n",
    "# You can use these callbacks to interact with your UI.\n",
    "#\n",
    "class MyListener(houndify.HoundListener):\n",
    "  def onPartialTranscript(self, transcript):\n",
    "    print(\"Partial transcript: \" + transcript)\n",
    "  def onFinalResponse(self, response):\n",
    "    print(\"Final response: \" + str(response))\n",
    "    \n",
    "    response_bytes = response[\"AllResults\"][0][\"ResponseAudioBytes\"]\n",
    "    os.system(\"say {}\".format(response_text)) \n",
    "    print(\"Response bytes:\" + response_bytes)\n",
    "    print(\"Final response: \" + str(response))\n",
    "\n",
    "    # convert bytes string to binary\n",
    "    decoded = base64.decodebytes(response_bytes.encode(\"ascii\"))\n",
    "\n",
    "    # writes response to wav file\n",
    "    with open('myfile.wav', mode='wb') as f:\n",
    "      f.write(decoded)\n",
    "    \n",
    "    va = sda.VideoAnimator(gpu=0, model_path=\"timit\")# Instantiate the animator\n",
    "    vid, aud = va(\"IMG_9997.bmp\", \"sabrina_audio.wav\")\n",
    "    va.save_video(vid, aud, \"generated_sabrina.mp4\")\n",
    "    \n",
    "    # open video\n",
    "      \n",
    "    \n",
    "  def onError(self, err):\n",
    "    print(\"Error: \" + str(err))\n",
    "\n",
    "\n",
    "client = houndify.StreamingHoundClient(CLIENT_ID, CLIENT_KEY, \"test_user\", enableVAD=False)\n",
    "client.setLocation(37.388309, -121.973968)\n",
    "client.setSampleRate(audio.getframerate())\n",
    "client.setHoundRequestInfo(\"ResponseAudioVoice\", \"Sharon\")\n",
    "client.setHoundRequestInfo(\"ResponseAudioShortOrLong\", \"Short\")\n",
    "\n",
    "\n",
    "client.start(MyListener())\n",
    "\n",
    "while True:\n",
    "  chunk_start = time.time()\n",
    "\n",
    "  samples = audio.readframes(BUFFER_SIZE)\n",
    "  if len(samples) == 0: break\n",
    "  if client.fill(samples): break\n",
    "\n",
    "  # Uncomment the line below to simulate real-time request\n",
    "  # time.sleep(chunk_duration - time.time() + chunk_start) \n",
    "\n",
    "result = client.finish() # returns either final response or error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/home/friend/notebooks/houndify.py\", line 362, in _callback\n",
      "    listener.onFinalResponse(parsedMsg)\n",
      "  File \"<ipython-input-8-feb52d76c9a9>\", line 14, in onFinalResponse\n",
      "    response_text = response[\"AllResults\"][0][\"SpokenResponse\"]\n",
      "KeyError: 'AllResults'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AudioLength': 0,\n",
       " 'BuildInfo': {'BuildNumber': '6107',\n",
       "  'Date': 'Sun Dec  8 20:17:31 PST 2019',\n",
       "  'Kind': 'Low Fat',\n",
       "  'Machine': '7fhpjh2.pnp.melodis.com',\n",
       "  'SVNBranch': 'dev_multi_machine',\n",
       "  'SVNRevision': '54997',\n",
       "  'User': 'knightly',\n",
       "  'Variant': 'release'},\n",
       " 'DomainUsage': [],\n",
       " 'Format': 'SoundHoundVoiceSearchResult',\n",
       " 'FormatVersion': '1.0',\n",
       " 'NumToReturn': 0,\n",
       " 'QueryID': '9a2e5e70-dc69-f817-52d2-d7e5b2e53120',\n",
       " 'ServerGeneratedId': '9a2e5e70-dc69-f817-52d2-d7e5b2e53120',\n",
       " 'Status': 'OK'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIENT_ID = \"Rco-d2ARwyAUSvklEH_ePg==\"\n",
    "CLIENT_KEY = \"V6TzSgIOkx0CW4AkBSxgLNrcHMEZ26PSB4uLaG-n8t6PsjZSEzBF-aGfXQv-mWXha0_grVB-TO5MGdKhx9XNFA==\"\n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "#\n",
    "# Simplest HoundListener; just print out what we receive.\n",
    "# You can use these callbacks to interact with your UI.\n",
    "#\n",
    "class MyListener(houndify.HoundListener):\n",
    "  def onPartialTranscript(self, transcript):\n",
    "    print(\"Partial transcript: \" + transcript)\n",
    "  def onFinalResponse(self, response):\n",
    "    # Playing the converted file \n",
    "    response_text = response[\"AllResults\"][0][\"SpokenResponse\"]\n",
    "    # get response audio bytes string\n",
    "    response_bytes = response[\"AllResults\"][0][\"ResponseAudioBytes\"]\n",
    "    os.system(\"say {}\".format(response_text)) \n",
    "    print(\"Response bytes:\" + response_bytes)\n",
    "    print(\"Final response: \" + str(response))\n",
    "\n",
    "    # convert bytes string to binary\n",
    "    decoded = base64.decodebytes(response_bytes.encode(\"ascii\"))\n",
    "\n",
    "    # writes response to wav file\n",
    "    with open('myfile.wav', mode='wb') as f:\n",
    "      f.write(decoded)\n",
    "    \n",
    "#     va = sda.VideoAnimator(gpu=0, model_path=\"timit\")# Instantiate the animator\n",
    "#     vid, aud = va(\"IMG_9997.bmp\", \"sabrina_audio.wav\")\n",
    "#     va.save_video(vid, aud, \"generated_sabrina.mp4\")\n",
    "      \n",
    "  \n",
    "  def onError(self, err):\n",
    "    print(\"Error: \" + str(err))\n",
    "\n",
    "\n",
    "client = houndify.StreamingHoundClient(CLIENT_ID, CLIENT_KEY, \"test_user\")\n",
    "client.setLocation(37.388309, -121.973968)\n",
    "client.setHoundRequestInfo(\"ResponseAudioVoice\", \"Sharon\")\n",
    "client.setHoundRequestInfo(\"ResponseAudioShortOrLong\", \"Short\")\n",
    "\n",
    "## Uncomment the lines below to see an example of using a custom\n",
    "## grammar for matching.  Use the file 'turnthelightson.wav' to try it.\n",
    "# clientMatches = [ {\n",
    "#   \"Expression\" : '([1/100 (\"can\"|\"could\"|\"will\"|\"would\").\"you\"].[1/10 \"please\"].(\"turn\"|\"switch\"|(1/100 \"flip\")).\"on\".[\"the\"].(\"light\"|\"lights\").[1/20 \"for\".\"me\"].[1/20 \"please\"])|([1/100 (\"can\"|\"could\"|\"will\"|\"would\").\"you\"].[1/10 \"please\"].[100 (\"turn\"|\"switch\"|(1/100 \"flip\"))].[\"the\"].(\"light\"|\"lights\").\"on\".[1/20 \"for\".\"me\"].[1/20 \"please\"])|(((\"i\".(\"want\"|\"like\"))|(((\"i\".[\"would\"])|(\"i\\'d\")).(\"like\"|\"want\"))).[\"the\"].(\"light\"|\"lights\").[\"turned\"|\"switched\"|(\"to\".\"go\")|(1/100\"flipped\")].\"on\".[1/20\"please\"])\"',\n",
    "#   \"Result\" : { \"Intent\" : \"TURN_LIGHT_ON\" },\n",
    "#   \"SpokenResponse\" : \"Ok, I\\'m turning the lights on.\",\n",
    "#   \"SpokenResponseLong\" : \"Ok, I\\'m turning the lights on.\",\n",
    "#   \"WrittenResponse\" : \"Ok, I\\'m turning the lights on.\",\n",
    "#   \"WrittenResponseLong\" : \"Ok, I\\'m turning the lights on.\"\n",
    "# } ]\n",
    "# client.setHoundRequestInfo('ClientMatches', clientMatches)\n",
    "\n",
    "client.start(MyListener())\n",
    "\n",
    "while True:\n",
    "  samples = sys.stdin.buffer.read(BUFFER_SIZE)\n",
    "  if len(samples) == 0: break\n",
    "  if client.fill(samples): break\n",
    "\n",
    "client.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./sample_stdin.py: not found\n",
      "\n",
      "Input File     : 'default' (alsa)\n",
      "Channels       : 2\n",
      "Sample Rate    : 48000\n",
      "Precision      : 16-bit\n",
      "Sample Encoding: 16-bit Signed Integer PCM\n",
      "\n",
      "In:0.00% 00:00:00.26 [00:00:00.00] Out:8.19k [      |      ]        Clip:0    sox FAIL sox: `-' error writing output file: Broken pipe\n",
      "In:0.00% 00:00:00.43 [00:00:00.00] Out:16.4k [      |      ]        Clip:0    rec FAIL sox: `-' error writing output file: Broken pipe\n",
      "Done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = sda.VideoAnimator(gpu=0, model_path=\"timit\")# Instantiate the animator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/scipy/signal/signaltools.py:2223: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/scipy/signal/signaltools.py:2225: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/scipy/signal/signaltools.py:2233: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] /= 2  # halve the component at -N/2\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/scipy/signal/signaltools.py:2234: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  temp = Y[sl]\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/scipy/signal/signaltools.py:2236: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = temp  # set that equal to the component at -N/2\n"
     ]
    }
   ],
   "source": [
    "vid, aud = va(\"IMG_9997.bmp\", \"sabrina_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "va.save_video(vid, aud, \"generated_sabrina.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
